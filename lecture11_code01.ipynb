{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Course\n",
    "### &nbsp; &nbsp; &nbsp; Xavier Bresson, Sept. 2016\n",
    "## Lecture 11 : Deep Learning 3 - Convolutional Neural Networks\n",
    "## Code 1 : LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of the original LeNet5 Convolutional Neural Networks:<br>\n",
    "Gradient-based learning applied to document recognition<br>\n",
    "Y LeCun, L Bottou, Y Bengio, P Haffner<br>\n",
    "Proceedings of the IEEE 86 (11), 2278-2324<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags # tf.app = a wrapper python-gflags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Data folder\n",
    "flags.DEFINE_string('dir_data', 'datasets', 'Directory to store data')\n",
    "flags.DEFINE_float('learning_rate', 0.2, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Batch size.')\n",
    "flags.DEFINE_float('regularization', 0.0, 'L2 regularizations of weights and biases.')\n",
    "flags.DEFINE_float('dropout', 1.0, 'Dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(FLAGS.dir_data, one_hot=False) # load data in local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000,)\n",
      "(5000, 784)\n",
      "(5000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = mnist.train.images.astype(np.float32)\n",
    "val_data = mnist.validation.images.astype(np.float32)\n",
    "test_data = mnist.test.images.astype(np.float32)\n",
    "train_labels = mnist.train.labels\n",
    "val_labels = mnist.validation.labels\n",
    "test_labels = mnist.test.labels\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define generic class of Neural Networks\n",
    "train_size = train_data.shape[0]\n",
    "\n",
    "class base_model(object):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.regularizers = 0 # L2 regularizers\n",
    "    \n",
    "    # Private methods\n",
    "    def _weight_variable(self, shape, regularization=False): \n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        var = tf.Variable(initial, name='weights')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)        \n",
    "        return var\n",
    "    \n",
    "    def _bias_variable(self, shape, regularization=False): \n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        var = tf.Variable(initial, name='bias')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var) \n",
    "        return var\n",
    "\n",
    "    def _conv2d(self, x, W):\n",
    "        return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "    def _max_pool_2x2(self, x):\n",
    "        return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    # Public methods\n",
    "    def loss(self, logits, labels, regularization): \n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        loss += regularization * self.regularizers     \n",
    "        #tf.scalar_summary('loss', loss) # Tensorboard     \n",
    "        return loss\n",
    "    \n",
    "    # Optimization\n",
    "    def training(self, loss, learning_rate, train_size, batch_size):            \n",
    "        # Optimizer: set up a variable that's incremented once per batch and\n",
    "        # controls the learning rate decay.\n",
    "        batch = tf.Variable(0)\n",
    "        # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                0.01,                # Base learning rate.\n",
    "                batch * batch_size,  # Current index into the dataset.\n",
    "                train_size,          # Decay step.\n",
    "                0.95,                # Decay rate.\n",
    "                staircase=True)\n",
    "        # Use simple momentum for the optimization.\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)  \n",
    "        train_op = optimizer.minimize(loss, global_step=batch) \n",
    "        return train_op\n",
    "    \n",
    "    def evaluation(self, logits, labels):\n",
    "        output_classes = tf.cast(tf.argmax(tf.nn.softmax(logits),1), tf.int32)\n",
    "        acc = 100.* tf.reduce_sum(tf.cast(tf.equal(output_classes,labels), tf.float32))/ tf.cast(tf.shape(logits)[0], tf.float32)\n",
    "        return acc  \n",
    "    \n",
    "    def prediction(self, logits):\n",
    "        \"\"\"Return the predicted classes.\"\"\"\n",
    "        output_classes = tf.cast(tf.argmax(tf.nn.softmax(logits),1), tf.int32)\n",
    "        return output_classes\n",
    "    \n",
    "# TensorBoard\n",
    "def variable_summaries(var, name):\n",
    "    with tf.name_scope(\"summaries\"):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Test: CL32-MP4-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEAT1 = 14*14 # Number of features\n",
    "NCLASSES = 10 # Number of classes\n",
    "\n",
    "class CNN_1CL_1MP_1FC(base_model):\n",
    "    \n",
    "    def __init__(self, K, F):\n",
    "        \n",
    "        print('CNN Architecture: 1CL+1MP+1FC')\n",
    "        super().__init__()\n",
    "        self.K = K  # Patch size\n",
    "        self.F = F  # Number of filters\n",
    "        self.W1 = self._weight_variable([self.K, self.K, 1, self.F], regularization=False)\n",
    "        self.b1 = self._bias_variable([self.F], regularization=False)\n",
    "        self.W2 = self._weight_variable([FEAT1*self.F, NCLASSES], regularization=True)        \n",
    "        self.b2 = self._bias_variable([NCLASSES], regularization=True)\n",
    "        \n",
    "    def inference(self, x, d):\n",
    "        \n",
    "        layer_name = 'CL32'\n",
    "        with tf.name_scope(layer_name):\n",
    "            # Grid filtering\n",
    "            x_2d = tf.reshape(x, [-1,28,28,1]) \n",
    "            y_2d = self._conv2d(x_2d, self.W1) + self.b1 \n",
    "            # Non-linear activation\n",
    "            y_2d = tf.nn.relu(y_2d)    \n",
    "            # Tensorboard\n",
    "            #variable_summaries(W, layer_name + '/W')\n",
    "            #variable_summaries(b, layer_name + '/bias')\n",
    "            #variable_summaries(x_2d, layer_name + '/x_2d')\n",
    "            #variable_summaries(y_2d, layer_name + '/y_2d')\n",
    "            \n",
    "        layer_name = 'MP4'\n",
    "        with tf.name_scope(layer_name):\n",
    "            # Max pooling\n",
    "            y_mp = self._max_pool_2x2(y_2d)\n",
    "            #variable_summaries(y_mp, layer_name + '/y_mp')\n",
    "            # Dropout\n",
    "            y_mp = tf.nn.dropout(y_mp, d)\n",
    "        \n",
    "        layer_name = 'FC10'\n",
    "        with tf.name_scope(layer_name):\n",
    "            y = tf.reshape(y_mp, [-1, FEAT1*self.F]) \n",
    "            y = tf.matmul(y, self.W2) + self.b2      \n",
    "            #variable_summaries(W, layer_name + '/W')\n",
    "            #variable_summaries(b, layer_name + '/b')\n",
    "            #variable_summaries(y, layer_name + '/y')\n",
    "            \n",
    "        return y   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5: CL32-MP4-CL64-MP4-FC512-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1=32 # Number of features of 1st CL layer\n",
    "F2=64 # Number of features of 2nd CL layer\n",
    "FEAT2 = 7*7* F2\n",
    "NFC1=512 # Number of nodes of 1st FC layer\n",
    "NCLASSES = 10 # Number of classes\n",
    "\n",
    "class CNN_LeNet5(base_model):\n",
    "    \n",
    "    def __init__(self, K):\n",
    "        \n",
    "        print('CNN Architecture: LeNet5')\n",
    "        super().__init__()\n",
    "        self.K = K  # Patch size\n",
    "        self.W1 = self._weight_variable([self.K, self.K, 1, F1], regularization=False)\n",
    "        self.b1 = self._bias_variable([F1], regularization=False)\n",
    "        self.W2 = self._weight_variable([self.K, self.K, F1, F2], regularization=False)\n",
    "        self.b2 = self._bias_variable([F2], regularization=False)\n",
    "        self.W3 = self._weight_variable([FEAT2, NFC1], regularization=True)\n",
    "        self.b3 = self._bias_variable([NFC1], regularization=True)\n",
    "        self.W4 = self._weight_variable([NFC1, NCLASSES], regularization=True)\n",
    "        self.b4 = self._bias_variable([NCLASSES], regularization=True)\n",
    "        \n",
    "    def inference(self, x, d):\n",
    "        \n",
    "        with tf.name_scope('CN32'):\n",
    "            # Grid filtering\n",
    "            x_2d = tf.reshape(x, [-1,28,28,1])\n",
    "            y_2d = self._conv2d(x_2d, self.W1) + self.b1\n",
    "            # Non-linear activation\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "            \n",
    "        with tf.name_scope('MP4'):\n",
    "            # Max pooling\n",
    "            y_mp = self._max_pool_2x2(y_2d)\n",
    "            \n",
    "        with tf.name_scope('CN64'):\n",
    "            # Grid filtering\n",
    "            y_2d = self._conv2d(y_mp, self.W2) + self.b2\n",
    "            # Non-linear activation\n",
    "            y_2d = tf.nn.relu(y_2d)\n",
    "            \n",
    "        with tf.name_scope('MP4'):\n",
    "            # Max pooling\n",
    "            y_mp = self._max_pool_2x2(y_2d)\n",
    "            \n",
    "        with tf.name_scope('FC512'):\n",
    "            y = tf.reshape(y_mp, [-1, FEAT2])\n",
    "            y = tf.matmul(y, self.W3) + self.b3\n",
    "            # Non-linear activation\n",
    "            y = tf.nn.relu(y)\n",
    "            # Dropout\n",
    "            y = tf.nn.dropout(y, d)\n",
    "                \n",
    "        with tf.name_scope('FC10'):\n",
    "            y = tf.matmul(y, self.W4) + self.b4\n",
    "            \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Architecture: 1CL+1MP+1FC\n"
     ]
    }
   ],
   "source": [
    "# Comment/uncomment\n",
    "NN_1Layer=False\n",
    "NN_1Layer=True\n",
    "if NN_1Layer==True:\n",
    "    model = CNN_1CL_1MP_1FC(K=5, F=10)\n",
    "    FLAGS.learning_rate = 0.05\n",
    "    FLAGS.regularization = 5e-4\n",
    "    FLAGS.dropout = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comment/uncomment\n",
    "NN_LeNet5=False\n",
    "#NN_LeNet5=True\n",
    "if NN_LeNet5==True:\n",
    "    model = CNN_LeNet5(K=5)\n",
    "    FLAGS.learning_rate = 0.05\n",
    "    FLAGS.regularization = 5e-4\n",
    "    FLAGS.dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs= 2 , train_size= 55000 , nb_iter= 1100\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_epochs = 10\n",
    "num_epochs = 2 # Early stop\n",
    "train_size = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * train_size) // FLAGS.batch_size\n",
    "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
    "\n",
    "# Construct computational graph\n",
    "x = tf.placeholder(tf.float32, (None, 784))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "d = tf.placeholder(tf.float32)\n",
    "logits = model.inference(x,d) # dropout activate\n",
    "loss = model.loss(logits, y, FLAGS.regularization)\n",
    "train_op = model.training(loss, FLAGS.learning_rate, train_size, FLAGS.batch_size)\n",
    "evaluation = model.evaluation(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs= 2 , nb_iter= 1100\n",
      "iter=0, freq_iter=10, training time: 0.00s, acc_train=-1.00, loss_train=-1.00\n",
      "iter=10, freq_iter=10, training time: 1.77s, acc_train=25.00, loss_train=2.26\n",
      "iter=100, acc_train=85.00, loss_train=0.57, acc_test=87.13, acc_test_nodropout=90.24, test time=14.87s\n",
      "iter=200, acc_train=86.00, loss_train=0.47, acc_test=91.29, acc_test_nodropout=93.61, test time=14.89s\n",
      "iter=300, acc_train=94.00, loss_train=0.30, acc_test=92.85, acc_test_nodropout=94.51, test time=15.27s\n",
      "iter=400, acc_train=97.00, loss_train=0.20, acc_test=93.81, acc_test_nodropout=95.12, test time=15.89s\n",
      "iter=500, acc_train=97.00, loss_train=0.24, acc_test=94.46, acc_test_nodropout=95.64, test time=15.96s\n",
      "iter=600, acc_train=94.00, loss_train=0.20, acc_test=94.75, acc_test_nodropout=96.05, test time=15.99s\n",
      "iter=700, acc_train=97.00, loss_train=0.18, acc_test=95.22, acc_test_nodropout=96.39, test time=16.29s\n",
      "iter=800, acc_train=98.00, loss_train=0.13, acc_test=95.38, acc_test_nodropout=96.56, test time=16.09s\n",
      "iter=900, acc_train=94.00, loss_train=0.21, acc_test=95.49, acc_test_nodropout=96.69, test time=16.00s\n",
      "iter=1000, acc_train=98.00, loss_train=0.11, acc_test=95.90, acc_test_nodropout=96.87, test time=16.16s\n",
      "iter=1100, acc_train=97.00, loss_train=0.14, acc_test=96.20, acc_test_nodropout=97.10, test time=16.24s\n",
      "final accuracy= 97.1\n",
      "Training time: 376.83s\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "\n",
    "# TensorFlow\n",
    "# Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "writer = tf.train.SummaryWriter('tmp/mnist_logs' + '/run1', sess.graph)\n",
    "op_summary = tf.merge_all_summaries()\n",
    "\n",
    "# Start\n",
    "sess.run(init)\n",
    "indices = collections.deque()\n",
    "tab_results = []\n",
    "tab_last_epoch = []\n",
    "start_last_epoch = nb_iter - train_size // FLAGS.batch_size\n",
    "nb_samples_last_epoch = 25\n",
    "freq_save_last_epoch = int(train_size // FLAGS.batch_size // (nb_samples_last_epoch-1))\n",
    "acc_train = -1.0\n",
    "loss_train = -1.0\n",
    "print('num_epochs=',num_epochs,', nb_iter=',nb_iter)\n",
    "t_start = time.process_time()\n",
    "for i in range(nb_iter):\n",
    "        \n",
    "    # Computational time\n",
    "    freq_iter = 10\n",
    "    if (i%freq_iter==0) & (i<=freq_iter):\n",
    "        print('iter={:d}, freq_iter={:d}, training time: {:.2f}s, acc_train={:2.2f}, loss_train={:2.2f}'\n",
    "              .format(i,freq_iter,time.process_time() - t_start,acc_train,loss_train))\n",
    "        t_start = time.process_time()\n",
    "         \n",
    "    # Generic batch extraction\n",
    "    if len(indices) < FLAGS.batch_size:\n",
    "        indices.extend(np.random.permutation(train_data.shape[0])) # rand permutation\n",
    "    idx = [indices.popleft() for i in range(FLAGS.batch_size)] # extract batch_size data\n",
    "    batch_xs, batch_ys = train_data[idx,:], train_labels[idx]\n",
    "    if type(batch_xs) is not np.ndarray:\n",
    "        batch_xs = batch_xs.toarray()  # convert to full matrices if sparse\n",
    "\n",
    "    # Run computational graph for weight learning\n",
    "    _,acc_train,loss_train = sess.run([train_op,evaluation,loss], feed_dict={x: batch_xs, y: batch_ys, d: FLAGS.dropout})\n",
    "    \n",
    "    # Display, save results\n",
    "    if (i+1)%100==0:\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        t_start_testset = time.process_time()\n",
    "        acc_test = sess.run(evaluation, feed_dict={x: mnist.test.images, y: mnist.test.labels, d: FLAGS.dropout})\n",
    "        acc_test_nodropout = sess.run(evaluation, feed_dict={x: mnist.test.images, y: mnist.test.labels, d: 1.0})\n",
    "        t_testset = time.process_time() - t_start_testset\n",
    "        \n",
    "        print('iter={:d}, acc_train={:2.2f}, loss_train={:2.2f}, acc_test={:2.2f}, acc_test_nodropout={:2.2f}, test time={:.2f}s'\n",
    "              .format(i+1,acc_train,loss_train,acc_test,acc_test_nodropout,t_testset))\n",
    "        \n",
    "        # Summaries for TensorBoard.\n",
    "        acc_train *= 1.0\n",
    "        acc_test *= 1.0\n",
    "        acc_test_nodropout *= 1.0\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='acc_train', simple_value=acc_train)\n",
    "        summary.value.add(tag='acc_test', simple_value=acc_test)\n",
    "        summary.value.add(tag='acc_test_nodropout', simple_value=acc_test_nodropout)\n",
    "        writer.add_summary(summary, i+1)\n",
    "        \n",
    "# Save accuracy for last batch       \n",
    "acc_test_nodropout = sess.run(evaluation, feed_dict={x: mnist.test.images, y: mnist.test.labels, d: 1.0})\n",
    "print('final accuracy=',acc_test_nodropout)\n",
    "        \n",
    "writer.close()  \n",
    "        \n",
    "print('Training time: {:.2f}s'.format(time.process_time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run TensorBoard:\n",
    "Go to folder of file lecture11_code01.ipynb<br>\n",
    "Open Terminal and type:<br>\n",
    "tensorboard --logdir='tmp/mnist_logs' â€”port 8889"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
