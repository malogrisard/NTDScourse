{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Course\n",
    "### &nbsp; &nbsp; &nbsp; Xavier Bresson, Sept. 2016\n",
    "## Lecture 11 : Deep Learning 3 - Convolutional Neural Networks\n",
    "## Code 2 : Convolutional Neural Networks for Graph-Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of CNNs for graph-structured data :<br>\n",
    "Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering<br>\n",
    "M Defferrard, X Bresson, P Vandergheynst<br>\n",
    "arXiv preprint arXiv:1606.09375<br>\n",
    "NIPS 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import collections\n",
    "import shutil\n",
    "import os\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto-reloading external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Import functions in lib folder\n",
    "import sys\n",
    "sys.path.insert(0, 'lib/')\n",
    "\n",
    "%aimport graph\n",
    "%aimport coarsening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags # tf.app = a wrapper python-gflags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Data folder\n",
    "flags.DEFINE_string('dir_data', 'datasets', 'Directory to store data')\n",
    "flags.DEFINE_float('learning_rate', 0.2, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('batch_size', 100, 'Batch size.')\n",
    "flags.DEFINE_float('regularization', 0.0, 'L2 regularizations of weights and biases.')\n",
    "flags.DEFINE_float('dropout', 1.0, 'Dropout')\n",
    "\n",
    "# Graphs\n",
    "flags.DEFINE_integer('number_edges', 8, 'Graph: minimum number of edges per vertex.')\n",
    "flags.DEFINE_string('metric', 'euclidean', 'Graph: similarity measure (between features).')\n",
    "# TODO: change cgcnn for combinatorial Laplacians.\n",
    "flags.DEFINE_bool('normalized_laplacian', True, 'Graph Laplacian: normalized.')\n",
    "flags.DEFINE_integer('coarsening_levels', 4, 'Number of coarsened graphs.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting datasets/train-images-idx3-ubyte.gz\n",
      "Extracting datasets/train-labels-idx1-ubyte.gz\n",
      "Extracting datasets/t10k-images-idx3-ubyte.gz\n",
      "Extracting datasets/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(FLAGS.dir_data, one_hot=False) # load data in local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000,)\n",
      "(5000, 784)\n",
      "(5000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = mnist.train.images.astype(np.float32)\n",
    "val_data = mnist.validation.images.astype(np.float32)\n",
    "test_data = mnist.test.images.astype(np.float32)\n",
    "train_labels = mnist.train.labels\n",
    "val_labels = mnist.validation.labels\n",
    "test_labels = mnist.test.labels\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)\n",
    "print(test_data.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Graph\n",
    "Data lie on a network/graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Graph of Euclidean grid\n",
    "def grid_graph(m, corners=False):\n",
    "    z = graph.grid(m)\n",
    "    dist, idx = graph.distance_sklearn_metrics(z, k=FLAGS.number_edges, metric=FLAGS.metric)\n",
    "    A = graph.adjacency(dist, idx)\n",
    "\n",
    "    # Connections are only vertical or horizontal on the grid.\n",
    "    # Corner vertices are connected to 2 neightbors only.\n",
    "    if corners:\n",
    "        import scipy.sparse\n",
    "        A = A.toarray()\n",
    "        A[A < A.max()/1.5] = 0\n",
    "        A = scipy.sparse.csr_matrix(A)\n",
    "        print('{} edges'.format(A.nnz))\n",
    "\n",
    "    print(\"{} > {} edges\".format(A.nnz//2, FLAGS.number_edges*m**2//2))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coarsening with Metis-like technique\n",
    "def coarsen(A, levels):\n",
    "    graphs, parents = coarsening.metis(A, levels)\n",
    "    perms = coarsening.compute_perm(parents)\n",
    "\n",
    "    laplacians = []\n",
    "    for i,A in enumerate(graphs):\n",
    "        M, M = A.shape\n",
    "\n",
    "        # No self-connections.\n",
    "        if True:\n",
    "            A = A.tocoo()\n",
    "            A.setdiag(0)\n",
    "\n",
    "        if i < levels:\n",
    "            A = coarsening.perm_adjacency(A, perms[i])\n",
    "\n",
    "        A = A.tocsr()\n",
    "        A.eliminate_zeros()\n",
    "        Mnew, Mnew = A.shape\n",
    "        print('Layer {0}: M_{0} = |V| = {1} nodes ({2} added), |E| = {3} edges'.format(i, Mnew, Mnew-M, A.nnz//2))\n",
    "\n",
    "        L = graph.laplacian(A, normalized=FLAGS.normalized_laplacian)\n",
    "        laplacians.append(L)\n",
    "    return laplacians, perms[0] if len(perms) > 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3198 > 3136 edges\n",
      "Layer 0: M_0 = |V| = 1056 nodes (272 added), |E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 528 nodes (112 added), |E| = 1428 edges\n",
      "Layer 2: M_2 = |V| = 264 nodes (42 added), |E| = 692 edges\n",
      "Layer 3: M_3 = |V| = 132 nodes (15 added), |E| = 331 edges\n",
      "Layer 4: M_4 = |V| = 66 nodes (0 added), |E| = 172 edges\n",
      "Execution time: 0.14s\n"
     ]
    }
   ],
   "source": [
    "# Construct graph\n",
    "t_start = time.process_time()\n",
    "A = grid_graph(28, corners=False)\n",
    "#A = graph.replace_random_edges(A, 0)\n",
    "\n",
    "# Compute coarsened graphs\n",
    "L, perm = coarsen(A, FLAGS.coarsening_levels)\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "\n",
    "# Compute max eigenvalue of graph Laplacians\n",
    "lmax = []\n",
    "lmax.append(graph.lmaxX(L[0]))\n",
    "lmax.append(graph.lmaxX(L[1]))\n",
    "lmax.append(graph.lmaxX(L[2]))\n",
    "lmax.append(graph.lmaxX(L[3]))\n",
    "#print(lmax[0],lmax[1],lmax[2],lmax[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 1056)\n",
      "(5000, 1056)\n",
      "(10000, 1056)\n",
      "Execution time: 1.59s\n"
     ]
    }
   ],
   "source": [
    "# Reindex nodes to satisfy a binary tree structure\n",
    "t_start = time.process_time()\n",
    "train_data = coarsening.perm_data(train_data, perm)\n",
    "val_data = coarsening.perm_data(val_data, perm)\n",
    "test_data = coarsening.perm_data(test_data, perm)\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "print('Execution time: {:.2f}s'.format(time.process_time() - t_start))\n",
    "#del perm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define generic class of Neural Networks\n",
    "class base_model(object):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.regularizers = 0 # L2 regularizers\n",
    "      \n",
    "    # Private methods\n",
    "    def _weight_variable(self, shape, regularization=True):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1) \n",
    "        var = tf.Variable(initial, name='weights')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)\n",
    "        #tf.histogram_summary(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _weight_variable_spectral(self, shape, regularization=True):\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1) \n",
    "        #initial = tf.abs(initial) # XAV\n",
    "        var = tf.Variable(initial, name='weights')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)\n",
    "        #tf.histogram_summary(var.op.name, var)\n",
    "        return var\n",
    "\n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        var = tf.Variable(initial, name='bias')\n",
    "        if regularization:\n",
    "            self.regularizers += tf.nn.l2_loss(var)\n",
    "        #tf.histogram_summary(var.op.name, var)\n",
    "        return var\n",
    "    \n",
    "    # Public methods\n",
    "    def loss(self, logits, labels, regularization): \n",
    "        labels = tf.to_int64(labels)\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        loss += regularization * self.regularizers     \n",
    "        #tf.scalar_summary('loss', loss) # Tensorboard     \n",
    "        return loss\n",
    "   \n",
    "    # Optimization\n",
    "    def training(self, loss, learning_rate, train_size, batch_size):            \n",
    "        # Optimizer: set up a variable that's incremented once per batch and\n",
    "        # controls the learning rate decay.\n",
    "        batch = tf.Variable(0)\n",
    "        # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "                0.01,                # Base learning rate.\n",
    "                batch * batch_size,  # Current index into the dataset.\n",
    "                train_size,          # Decay step.\n",
    "                0.95,                # Decay rate.\n",
    "                staircase=True)\n",
    "        # Use simple momentum for the optimization.\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)  \n",
    "        train_op = optimizer.minimize(loss, global_step=batch) \n",
    "        return train_op\n",
    "    \n",
    "    def evaluation(self, logits, labels):\n",
    "        output_classes = tf.cast(tf.argmax(tf.nn.softmax(logits),1), tf.int32)\n",
    "        acc = 100.* tf.reduce_sum(tf.cast(tf.equal(output_classes,labels), tf.float32))/ tf.cast(tf.shape(logits)[0], tf.float32)\n",
    "        return acc\n",
    "\n",
    "    def prediction(self, logits):\n",
    "        \"\"\"Return the predicted classes.\"\"\"\n",
    "        output_classes = tf.cast(tf.argmax(tf.nn.softmax(logits),1), tf.int32)\n",
    "        return output_classes\n",
    "    \n",
    "    def mpool1(self, x, p):\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x        \n",
    "\n",
    "    def chebyshev5(self, x, L, lmax, Fout, K, W):\n",
    "        N, M, Fin = x.get_shape()\n",
    "        N, M, Fin = int(N), int(M), int(Fin) \n",
    "        L = graph.rescale_L(L, lmax) \n",
    "        L = L.tocoo() \n",
    "        indices = np.column_stack((L.row, L.col)) \n",
    "        L = tf.SparseTensor(indices, L.data, L.shape) \n",
    "        L = tf.sparse_reorder(L) \n",
    "        # Transform to Chebyshev basis\n",
    "        x0 = tf.transpose(x, perm=[1, 2, 0])  # M x Fin x N \n",
    "        x0 = tf.reshape(x0, [M, Fin*N])  # M x Fin*N\n",
    "        x = tf.expand_dims(x0, 0)  # 1 x M x Fin*N\n",
    "        def concat(x, x_):\n",
    "            x_ = tf.expand_dims(x_, 0)  # 1 x M x Fin*N\n",
    "            return tf.concat(0, [x, x_])  # K x M x Fin*N        \n",
    "        if K > 1: \n",
    "            x1 = tf.sparse_tensor_dense_matmul(L, x0) \n",
    "            x = concat(x, x1) \n",
    "        for k in range(2, K):\n",
    "            x2 = 2 * tf.sparse_tensor_dense_matmul(L, x1) - x0  # M x Fin*N\n",
    "            x = concat(x, x2)\n",
    "            x0, x1 = x1, x2      \n",
    "        x = tf.reshape(x, [K, M, Fin, N])  # K x M x Fin x N\n",
    "        x = tf.transpose(x, perm=[3,1,2,0])  # N x M x Fin x K\n",
    "        x = tf.reshape(x, [N*M, Fin*K])  # N*M x Fin*K\n",
    "        # Filter: Fin*Fout filters of order K, i.e. one filterbank per feature.\n",
    "        x = tf.matmul(x, W)  # N*M x Fout\n",
    "        out = tf.reshape(x, [N, M, Fout])  # N x M x Fout\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Test: CL32-MP4-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1 = 32 # Number of features\n",
    "K1 = 20 # Spatial support = polynomial order\n",
    "NCLASSES = 10 # Number of classes\n",
    "M0 = L[0].shape[0]\n",
    "    \n",
    "class GCNN_1CL_1MP_1FC(base_model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        print('Graph CNN Architecture: 1CL+1MP+1FC')\n",
    "        super().__init__()\n",
    "        self.W1 = self._weight_variable_spectral([K1, F1], regularization=False)\n",
    "        self.b1 = self._bias_variable([1, 1, F1], regularization=False)\n",
    "        self.W2 = self._weight_variable([int(M0/4*F1), NCLASSES], regularization=True)\n",
    "        self.b2 = self._bias_variable([NCLASSES], regularization=True)\n",
    "        \n",
    "    def inference(self, x, L, lmax, d):\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        layer_name = 'CN32'\n",
    "        with tf.name_scope(layer_name):\n",
    "            # Graph filtering\n",
    "            x = tf.expand_dims(x, 2)  # N x M x F=1 \n",
    "            x = self.chebyshev5(x, L[0], lmax[0], F1, K1, self.W1)           \n",
    "            x = x + self.b1            \n",
    "            # Non-linear activation\n",
    "            x = tf.nn.relu(x)\n",
    "    \n",
    "        # Max pooling\n",
    "        layer_name = 'MP4' \n",
    "        with tf.name_scope(layer_name):\n",
    "            x = self.mpool1(x,4)\n",
    "            # Dropout\n",
    "            x = tf.nn.dropout(x, d)      \n",
    "            \n",
    "        # Fully connected hidden layers\n",
    "        layer_name = 'FC10'\n",
    "        with tf.name_scope(layer_name):\n",
    "            N, M, F = x.get_shape()\n",
    "            x = tf.reshape(x, [int(N), int(M0/4*F1)])  # N x M   \n",
    "            x = tf.matmul(x, self.W2) + self.b2\n",
    "                    \n",
    "        return x       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5: CL32-MP4-CL64-MP4-FC512-FC10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F1 = 32 # Number of features of 1st CL layer\n",
    "K1 = 20 # Spatial support = polynomial order of 1st CL layer\n",
    "F2 = 64 # Number of features of 2nd CL layer\n",
    "K2 = 20 # Spatial support = polynomial order of 2nd CL layer\n",
    "NFC = 512 # Number of nodes for 1st FC layer\n",
    "NCLASSES = 10 # Number of classes\n",
    "M0 = L[0].shape[0]\n",
    "    \n",
    "class GCNN_LeNet5(base_model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        print('Graph CNN Architecture: LeNet5')\n",
    "        #print(K1,K2)\n",
    "        super().__init__()\n",
    "        self.W1 = self._weight_variable_spectral([K1, F1], regularization=False)\n",
    "        self.b1 = self._bias_variable([1, 1, F1], regularization=False)\n",
    "        self.W2 = self._weight_variable_spectral([F1*K1, F2], regularization=False)\n",
    "        self.b2 = self._bias_variable([1, 1, F2], regularization=False)\n",
    "        self.W3 = self._weight_variable([int(M0/16*F2), NFC], regularization=True)\n",
    "        self.b3 = self._bias_variable([NFC], regularization=True)\n",
    "        self.W4 = self._weight_variable([NFC, NCLASSES], regularization=True)\n",
    "        self.b4 = self._bias_variable([NCLASSES], regularization=True)\n",
    "        \n",
    "    def inference(self, x, L, lmax, d):\n",
    "        \n",
    "        # Graph convolutional layers\n",
    "        layer_name = 'CN32' \n",
    "        with tf.name_scope(layer_name):\n",
    "            # Graph filtering\n",
    "            x = tf.expand_dims(x, 2)  # N x M x F=1 \n",
    "            x = self.chebyshev5(x, L[0], lmax[0], F1, K1, self.W1)\n",
    "            x = x + self.b1\n",
    "            # Non-linear activation\n",
    "            x = tf.nn.relu(x)\n",
    "    \n",
    "        # Max pooling\n",
    "        layer_name = 'MP4' \n",
    "        with tf.name_scope(layer_name):\n",
    "            x = self.mpool1(x,4)\n",
    "            \n",
    "        # Graph convolutional layer\n",
    "        layer_name = 'CN64' \n",
    "        with tf.name_scope(layer_name):\n",
    "            # Graph filtering\n",
    "            x = self.chebyshev5(x, L[2], lmax[2], F2, K2, self.W2) \n",
    "            x = x + self.b2\n",
    "            # Non-linear activation\n",
    "            x = tf.nn.relu(x)\n",
    "            \n",
    "        # Max pooling\n",
    "        layer_name = 'MP4' \n",
    "        with tf.name_scope(layer_name):\n",
    "            x = self.mpool1(x,4)\n",
    "            \n",
    "        # Fully connected hidden layer\n",
    "        layer_name = 'FC512'\n",
    "        with tf.name_scope(layer_name):\n",
    "            N, M, F = x.get_shape()\n",
    "            x = tf.reshape(x, [int(N), int(M0/16*F2)])  # N x M_0/4*F2\n",
    "            x = tf.matmul(x, self.W3) + self.b3 \n",
    "            x = tf.nn.relu(x)           \n",
    "            # Dropout\n",
    "            x = tf.nn.dropout(x, d)\n",
    "                \n",
    "        layer_name = 'FC10'\n",
    "        with tf.name_scope(layer_name):\n",
    "            x = tf.matmul(x, self.W4) + self.b4 \n",
    "                        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Architecture: 1CL+1MP+1FC\n",
      "0.05 0.05 0.75\n"
     ]
    }
   ],
   "source": [
    "# Comment/uncomment\n",
    "NN_1Layer=False\n",
    "NN_1Layer=True\n",
    "if NN_1Layer==True:\n",
    "    model = GCNN_1CL_1MP_1FC()\n",
    "    FLAGS.learning_rate = 0.05\n",
    "    FLAGS.regularization = 5e-2\n",
    "    FLAGS.dropout = 0.75\n",
    "    print(FLAGS.learning_rate,FLAGS.regularization,FLAGS.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comment/uncomment\n",
    "NN_LeNet5=False\n",
    "#NN_LeNet5=True\n",
    "if NN_LeNet5==True:\n",
    "    model = GCNN_LeNet5()\n",
    "    FLAGS.learning_rate = 0.5\n",
    "    FLAGS.regularization = 5e-4\n",
    "    FLAGS.dropout = 0.5\n",
    "    print(FLAGS.learning_rate,FLAGS.regularization,FLAGS.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs= 2 , train_size= 55000 , nb_iter= 1100\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_epochs = 10\n",
    "num_epochs = 2 # Early stop\n",
    "train_size = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * train_size) // FLAGS.batch_size\n",
    "print('num_epochs=',num_epochs,', train_size=',train_size,', nb_iter=',nb_iter)\n",
    "\n",
    "# Construct computational graph\n",
    "x = tf.placeholder(tf.float32, (FLAGS.batch_size, L[0].shape[0]))\n",
    "y = tf.placeholder(tf.int32, (FLAGS.batch_size))\n",
    "d = tf.placeholder(tf.float32)\n",
    "logits = model.inference(x,L,lmax,d) \n",
    "loss = model.loss(logits, y, FLAGS.regularization)\n",
    "train_op = model.training(loss, FLAGS.learning_rate, train_size, FLAGS.batch_size)\n",
    "evaluation = model.evaluation(logits, y)\n",
    "prediction = model.prediction(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1056)\n",
      "(200, 1056)\n"
     ]
    }
   ],
   "source": [
    "# Faster test evaluation\n",
    "test_data_evaluation = test_data\n",
    "test_labels_evaluation = test_labels\n",
    "if 1==1:\n",
    "    print(test_data.shape)\n",
    "    nb_selected = 1200\n",
    "    nb_selected = 200\n",
    "    test_data_evaluation = test_data[:nb_selected,:]\n",
    "    test_labels_evaluation = test_labels[:nb_selected]\n",
    "print(test_data_evaluation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs= 2 , nb_iter= 1100\n",
      "iter=0, freq_iter=10, training time: 0.00s, acc_train=-1.00, loss_train=-1.00\n",
      "iter=10, freq_iter=10, training time: 6.13s, acc_train=15.00, loss_train=18.09\n",
      "iter=100, acc_train=87.00, loss_train=6.96, acc_test=86.00, acc_test_nodropout=89.50, test time=1.52s\n",
      "iter=200, acc_train=83.00, loss_train=2.80, acc_test=92.00, acc_test_nodropout=92.50, test time=1.45s\n",
      "iter=300, acc_train=89.00, loss_train=1.31, acc_test=90.50, acc_test_nodropout=92.00, test time=1.55s\n",
      "iter=400, acc_train=87.00, loss_train=0.90, acc_test=90.50, acc_test_nodropout=91.50, test time=1.42s\n",
      "iter=500, acc_train=87.00, loss_train=0.63, acc_test=88.00, acc_test_nodropout=90.00, test time=1.50s\n",
      "iter=600, acc_train=84.00, loss_train=0.57, acc_test=89.00, acc_test_nodropout=93.00, test time=1.43s\n",
      "iter=700, acc_train=86.00, loss_train=0.62, acc_test=91.00, acc_test_nodropout=91.00, test time=1.52s\n",
      "iter=800, acc_train=88.00, loss_train=0.68, acc_test=88.00, acc_test_nodropout=89.00, test time=1.46s\n",
      "iter=900, acc_train=90.00, loss_train=0.44, acc_test=92.50, acc_test_nodropout=92.00, test time=1.44s\n",
      "iter=1000, acc_train=94.00, loss_train=0.35, acc_test=90.50, acc_test_nodropout=90.00, test time=1.42s\n",
      "iter=1100, acc_train=91.00, loss_train=0.48, acc_test=91.50, acc_test_nodropout=91.50, test time=1.49s\n",
      "final accuracy= 91.5\n",
      "Training time: 679.35s\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "\n",
    "# TensorFlow\n",
    "# Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "writer = tf.train.SummaryWriter('tmp/mnist_logs' + '/run2', sess.graph)\n",
    "op_summary = tf.merge_all_summaries()\n",
    "\n",
    "# Start\n",
    "sess.run(init)\n",
    "indices = collections.deque() \n",
    "tab_results = []\n",
    "tab_last_epoch = []\n",
    "start_last_epoch = nb_iter - train_size // FLAGS.batch_size\n",
    "nb_samples_last_epoch = 25 \n",
    "freq_save_last_epoch = int(train_size // FLAGS.batch_size // (nb_samples_last_epoch-1))\n",
    "acc_train = -1.0\n",
    "loss_train = -1.0\n",
    "print('num_epochs=',num_epochs,', nb_iter=',nb_iter)\n",
    "t_start = time.process_time()\n",
    "for i in range(nb_iter): \n",
    "    \n",
    "    # Computational time\n",
    "    freq_iter = 10 \n",
    "    if (i%freq_iter==0) & (i<=freq_iter):\n",
    "        print('iter={:d}, freq_iter={:d}, training time: {:.2f}s, acc_train={:2.2f}, loss_train={:2.2f}'\n",
    "              .format(i,freq_iter,time.process_time() - t_start,acc_train,loss_train))\n",
    "        t_start = time.process_time()\n",
    "        \n",
    "    # Generic batch extraction\n",
    "    if len(indices) < FLAGS.batch_size:\n",
    "        indices.extend(np.random.permutation(train_data.shape[0])) # rand permutation\n",
    "    idx = [indices.popleft() for i in range(FLAGS.batch_size)] # extract batch_size data\n",
    "    batch_xs, batch_ys = train_data[idx,:], train_labels[idx]\n",
    "    if type(batch_xs) is not np.ndarray:\n",
    "        batch_xs = batch_xs.toarray()  # convert to full matrices if sparse\n",
    "\n",
    "    # Run computational graph for weight learning\n",
    "    _,acc_train,loss_train = sess.run([train_op,evaluation,loss], feed_dict={x: batch_xs, y: batch_ys, d: FLAGS.dropout})    \n",
    "    \n",
    "    # Display, save results\n",
    "    if (i+1)%100==0: \n",
    "        \n",
    "        t_start_testset = time.process_time()\n",
    "        \n",
    "        # Compute test accuracy\n",
    "        data = test_data_evaluation\n",
    "        size = data.shape[0]\n",
    "        predictions = np.zeros(size, dtype='int32')\n",
    "        predictions_nodropout = np.zeros(size, dtype='int32')\n",
    "        for begin in range(0, size, FLAGS.batch_size):\n",
    "            end = begin + FLAGS.batch_size\n",
    "            end = min([end, size])\n",
    "            batch_data = np.zeros((FLAGS.batch_size, data.shape[1]))\n",
    "            tmp_data = data[begin:end,:]\n",
    "            if type(tmp_data) is not np.ndarray:\n",
    "                tmp_data = tmp_data.toarray()  \n",
    "            batch_data[:end-begin] = tmp_data\n",
    "            batch_label = np.zeros((FLAGS.batch_size))\n",
    "            batch_pred = np.zeros((FLAGS.batch_size))\n",
    "            batch_pred = sess.run(prediction, feed_dict= {x: batch_data, y: batch_label, d: FLAGS.dropout}) # for test set, comment to speedup computations \n",
    "            predictions[begin:end] = batch_pred[:end-begin]\n",
    "            batch_pred_nodropout = sess.run(prediction, feed_dict= {x: batch_data, y: batch_label, d: 1.0})\n",
    "            predictions_nodropout[begin:end] = batch_pred_nodropout[:end-begin]   \n",
    "        acc_test = 100.* float( sum(predictions == test_labels_evaluation) ) / float(size)\n",
    "        acc_test_nodropout = 100.* float( sum(predictions_nodropout == test_labels_evaluation) ) / float(size)\n",
    "        \n",
    "        t_testset = time.process_time() - t_start_testset\n",
    "        \n",
    "        print('iter={:d}, acc_train={:2.2f}, loss_train={:2.2f}, acc_test={:2.2f}, acc_test_nodropout={:2.2f}, test time={:.2f}s'\n",
    "              .format(i+1,acc_train,loss_train,acc_test,acc_test_nodropout,t_testset))\n",
    "        \n",
    "        # Summaries for TensorBoard.\n",
    "        acc_train *= 1.0\n",
    "        acc_test *= 1.0\n",
    "        acc_test_nodropout *= 1.0\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag='acc_train', simple_value=acc_train)\n",
    "        summary.value.add(tag='acc_test', simple_value=acc_test)\n",
    "        summary.value.add(tag='acc_test_nodropout', simple_value=acc_test_nodropout)\n",
    "        writer.add_summary(summary, i+1)\n",
    "         \n",
    "# Save accuracy for last batch\n",
    "data = test_data_evaluation\n",
    "size = data.shape[0]\n",
    "predictions_nodropout = np.zeros(size, dtype='int32')\n",
    "for begin in range(0, size, FLAGS.batch_size):\n",
    "    end = begin + FLAGS.batch_size\n",
    "    end = min([end, size])\n",
    "    batch_data = np.zeros((FLAGS.batch_size, data.shape[1]))\n",
    "    tmp_data = data[begin:end,:]\n",
    "    if type(tmp_data) is not np.ndarray:\n",
    "        tmp_data = tmp_data.toarray()  # convert sparse matrices\n",
    "    batch_data[:end-begin] = tmp_data\n",
    "    batch_label = np.zeros((FLAGS.batch_size))\n",
    "    batch_pred = sess.run(prediction, feed_dict= {x: batch_data, y: batch_label, d: FLAGS.dropout})\n",
    "    predictions[begin:end] = batch_pred[:end-begin]\n",
    "    batch_pred_nodropout = sess.run(prediction, feed_dict= {x: batch_data, y: batch_label, d: 1.0})\n",
    "    predictions_nodropout[begin:end] = batch_pred_nodropout[:end-begin]   \n",
    "acc_test_nodropout = 100.* float( sum(predictions_nodropout == test_labels_evaluation) ) / float(size)\n",
    "print('final accuracy=',acc_test_nodropout)\n",
    "                \n",
    "writer.close()  \n",
    "        \n",
    "print('Training time: {:.2f}s'.format(time.process_time() - t_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
