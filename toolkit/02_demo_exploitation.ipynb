{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Python Tour of Data Science: Data Exploitation\n",
    "\n",
    "[MichaÃ«l Defferrard](http://deff.ch), *PhD student*, [EPFL](http://epfl.ch) [LTS2](http://lts2.epfl.ch)\n",
    "\n",
    "The data `X.npy` and `y.npy` can be obtained by running the [data acquisition and exploration demo](01_demo_acquisition_exploration.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cross-platform (Windows / Mac / Linux) paths.\n",
    "import os.path\n",
    "folder = os.path.join('..', 'data', 'credit_card_defaults')\n",
    "\n",
    "import numpy as np\n",
    "X = np.load(os.path.join(folder, 'X.npy'))\n",
    "y = np.load(os.path.join(folder, 'y.npy'))\n",
    "n, d = X.shape\n",
    "print('The data is a {} with {} samples of dimensionality {}.'.format(type(X), n, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Pre-Processing\n",
    "\n",
    "Back to [NumPy](http://www.numpy.org/), the fundamental package for scientific computing with Python. It provides multi-dimensional arrays, data types and linear algebra routines. Note that [scikit-learn](http://scikit-learn.org) provides many helpers for those tasks.\n",
    "\n",
    "Pre-processing usually consists of:\n",
    "1. Data types transformation. The data has not necessarilly the format the chosen learning algorithm expects. This was done in the previous notebook before doing statistics with `statsmodels`.\n",
    "1. Data normalization. Some algorithms expect data to be centered and scaled. Some will train faster.\n",
    "1. Data randomization. If the samples are presented in sequence, it'll train faster if they are not correlated.\n",
    "1. Train / test splitting. You may have to be careful here, e.g. not including future events in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Center and scale.\n",
    "# Note: on a serious project, should be done after train / test split.\n",
    "X = X.astype(np.float)\n",
    "X -= X.mean(axis=0)\n",
    "X /= X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training and testing sets.\n",
    "test_size = 10000\n",
    "print('Split: {} testing and {} training samples'.format(test_size, y.size - test_size))\n",
    "perm = np.random.permutation(y.size)\n",
    "X_test  = X[perm[:test_size]]\n",
    "X_train = X[perm[test_size:]]\n",
    "y_test  = y[perm[:test_size]]\n",
    "y_train = y[perm[test_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 A first Predictive Model\n",
    "\n",
    "The ingredients of a Machine Learning (ML) model are:\n",
    "1. A predictive function, e.g. the linear transformation $f(x) = x^Tw + b$.\n",
    "1. An error function, e.g. the least squares $E = \\sum_{i=1}^n \\left( f(x_i) - y_i \\right)^2 = \\| f(X) - y \\|_2^2$.\n",
    "1. An optional regularization, e.g. the Thikonov regularization $R = \\|w\\|_2^2$.\n",
    "1. Which makes up the loss / objective function $L = E + \\alpha R$.\n",
    "\n",
    "Our model has a sole hyper-parameter, $\\alpha \\geq 0$, which controls the shrinkage.\n",
    "\n",
    "A Machine Learning (ML) problem can often be cast as a (convex or smooth) optimization problem which objective is to find the parameters (here $w$ and $b$) who minimize the loss, e.g.\n",
    "$$\\hat{w}, \\hat{b} = \\operatorname*{arg min}_{w,b} L = \\operatorname*{arg min}_{w,b} \\| Xw + b - y \\|_2^2 + \\alpha \\|w\\|_2^2.$$\n",
    "\n",
    "If the problem is convex and smooth, one can compute the gradients\n",
    "$$\\frac{\\partial L}{\\partial{w}} = 2 X^T (Xw+b-y) + 2\\alpha w,$$\n",
    "$$\\frac{\\partial L}{\\partial{b}} = 2 \\sum_{i=1}^n (x_i^Tw+b-y_i) = 2 \\sum_{i=1}^n (x_i^Tw-y_i) + 2n \\cdot b,$$\n",
    "\n",
    "which can be used in a [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) scheme or to form closed-form solutions:\n",
    "$$\\frac{\\partial L}{\\partial{w}} = 0 \\ \\rightarrow \\ 2 X^T X\\hat{w} + 2\\alpha \\hat{w} = 2 X^T y - 2 X^T b \\ \\rightarrow \\ \\hat{w} = (X^T X + \\alpha I)^{-1} X^T (y-b),$$\n",
    "$$\\frac{\\partial L}{\\partial{b}} = 0 \\ \\rightarrow \\ 2n\\hat{b} = 2\\sum_{i=1}^n (y_i) - \\underbrace{2\\sum_{i=1}^n (x_i^Tw)}_{=0 \\text{ if centered}} \\ \\rightarrow \\ \\hat{b} = \\frac1n I^T y = \\operatorname{mean}(y).$$\n",
    "\n",
    "What if the resulting problem is non-smooth ? See the [PyUNLocBoX](http://pyunlocbox.readthedocs.io), a convex optimization toolbox which implements [proximal splitting methods](https://en.wikipedia.org/wiki/Proximal_gradient_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Take a *symbolic* Derivative\n",
    "\n",
    "Let's verify our manually derived gradients ! [SymPy](http://www.sympy.org/) is our computer algebra system (CAS) (like [Mathematica](https://www.wolfram.com/mathematica), [Maple](https://www.maplesoft.com/products/Maple)) of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "sp.init_printing()\n",
    "\n",
    "X, y, w, b, a = sp.symbols('x y w b a')\n",
    "L = (X*w + b - y)**2 + a*w**2\n",
    "\n",
    "dLdw = sp.diff(L, w)\n",
    "dLdb = sp.diff(L, b)\n",
    "\n",
    "from IPython.display import display\n",
    "display(L)\n",
    "display(dLdw)\n",
    "display(dLdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build the Classifier\n",
    "\n",
    "Relying on the derived equations, we can implement our model relying only on the [NumPy](http://www.numpy.org/) linear algebra capabilities (really wrappers to [BLAS](http://www.netlib.org/blas) / [LAPACK](http://www.netlib.org/lapack) implementations such as [ATLAS](http://math-atlas.sourceforge.net), [OpenBLAS](http://www.openblas.net) or [MKL](https://software.intel.com/intel-mkl)).\n",
    "\n",
    "A ML model is best represented as a class, with hyper-parameters and parameters stored as attributes, and is composed of two essential methods:\n",
    "1. `y_pred = model.predict(X_test)`: return the predictions $y$ given the features $X$.\n",
    "1. `model.fit(X_train, y_train)`: learn the model parameters such as to predict $y$ given $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RidgeRegression(object):\n",
    "    \"\"\"Our ML model.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        \"The class' constructor. Initialize the hyper-parameters.\"\n",
    "        self.a = alpha\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the predicted class given the features.\"\"\"\n",
    "        return np.sign(X.dot(self.w) + self.b)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Learn the model's parameters given the training data, the closed-form way.\"\"\"\n",
    "        n, d = X.shape\n",
    "        self.b = np.mean(y)\n",
    "        Ainv = np.linalg.inv(X.T.dot(X) + self.a * np.identity(d))\n",
    "        self.w = Ainv.dot(X.T).dot(y - self.b)\n",
    "\n",
    "    def loss(self, X, y, w=None, b=None):\n",
    "        \"\"\"Return the current loss.\n",
    "        This method is not strictly necessary, but it provides\n",
    "        information on the convergence of the learning process.\"\"\"\n",
    "        w = self.w if w is None else w  # The ternary conditional operator\n",
    "        b = self.b if b is None else b  # makes those tests concise.\n",
    "        import autograd.numpy as np  # See below for autograd.\n",
    "        return np.linalg.norm(np.dot(X, w) + b - y)**2 + self.a * np.linalg.norm(w, 2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model can learn its parameters and predict targets, it's time to evaluate it. Our metric for binary classification is the accuracy, which gives the percentage of correcly classified test samples. Depending on the application, the time spent for inference or training might also be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Our evaluation metric, the classification accuracy.\"\"\"\n",
    "    return np.sum(y_pred == y_true) / y_true.size\n",
    "\n",
    "def evaluate(model):\n",
    "    \"\"\"Helper function to instantiate, train and evaluate the model.\n",
    "    It returns the classification accuracy, the loss and the execution time.\"\"\"\n",
    "    import time\n",
    "    t = time.process_time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    loss = model.loss(X_test, y_test)\n",
    "    t = time.process_time() - t\n",
    "    print('accuracy: {:.2f}%, loss: {:.2f}, time: {:.2f}ms'.format(acc*100, loss, t*1000))\n",
    "    return model\n",
    "\n",
    "alpha = 1e-2*n\n",
    "model = RidgeRegression(alpha)\n",
    "evaluate(model)\n",
    "\n",
    "models = []\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we got around 80% accuracy with such a simple model ! Inference and training time looks good.\n",
    "\n",
    "For those of you who don't now about numerical mathematics, solving a linear system of equations by inverting a matrix can be numerically instable. Let's do it the proper way and use a proper solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fit_lapack(self, X, y):\n",
    "    \"\"\"Better way (numerical stability): solve the linear system with LAPACK.\"\"\"\n",
    "    n, d = X.shape\n",
    "    self.b = np.mean(y)\n",
    "    A = X.T.dot(X) + self.a * np.identity(d)\n",
    "    b = X.T.dot(y - self.b)\n",
    "    self.w = np.linalg.solve(A, b)\n",
    "\n",
    "# Let's monkey patch our object (Python is a dynamic language).\n",
    "RidgeRegression.fit = fit_lapack\n",
    "\n",
    "# Yeah just to be sure.\n",
    "models.append(evaluate(RidgeRegression(alpha)))\n",
    "assert np.allclose(models[-1].w, models[0].w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Learning as Gradient Descent\n",
    "\n",
    "Descending the gradient of our objective will lead us to a local minimum. If the objective is convex, that minimum will be global. Let's implement the gradient computed above and a simple gradient descent algorithm\n",
    "$$w^{(t+1)} = w^{(t)} - \\gamma \\frac{\\partial L}{\\partial w}$$\n",
    "where $\\gamma$ is the learning rate, another hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RidgeRegressionGradient(RidgeRegression):\n",
    "    \"\"\"This model inherits from `ridge_regression`. We overload the constructor, add a gradient\n",
    "    function and replace the learning algorithm, but don't touch the prediction and loss functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0, rate=0.1, niter=1000):\n",
    "        \"\"\"Here are new hyper-parameters: the learning rate and the number of iterations.\"\"\"\n",
    "        super().__init__(alpha)\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "        \n",
    "    def grad(self, X, y, w):\n",
    "        A = X.dot(w) + self.b - y\n",
    "        return 2 * X.T.dot(A) + 2 * self.a * w\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        self.b = np.mean(y)\n",
    "        \n",
    "        self.w = np.random.normal(size=d)\n",
    "        for i in range(self.niter):\n",
    "            self.w -= self.rate * self.grad(X, y, self.w)\n",
    "            \n",
    "            # Show convergence.\n",
    "            if i % (self.niter//10) == 0:\n",
    "                print('loss at iteration {}: {:.2f}'.format(i, self.loss(X, y)))\n",
    "            \n",
    "models.append(evaluate(RidgeRegressionGradient(alpha, 1e-6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tyred of derivating gradients by hand ? Welcome [autograd](https://github.com/HIPS/autograd/), our tool of choice for [automatic differentation](https://en.wikipedia.org/wiki/Automatic_differentiation). Alternatives are [Theano](http://deeplearning.net/software/theano/) and [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RidgeRegressionAutograd(RidgeRegressionGradient):\n",
    "    \"\"\"Here we derive the gradient during construction and update the gradient function.\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        from autograd import grad\n",
    "        self.grad = grad(self.loss, argnum=2)\n",
    "\n",
    "models.append(evaluate(RidgeRegressionAutograd(alpha, 1e-6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Learning as generic Optimization\n",
    "\n",
    "Sometimes we don't want to implement the optimization by hand and would prefer a generic optimization algorithm. Let's make use of [SciPy](https://www.scipy.org/), which provides high-level algorithms for, e.g. [optimization](http://docs.scipy.org/doc/scipy/reference/optimize.html), [statistics](http://docs.scipy.org/doc/scipy/reference/stats.html), [interpolation](http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html), [signal processing](http://docs.scipy.org/doc/scipy/reference/tutorial/signal.html), [sparse matrices](http://docs.scipy.org/doc/scipy/reference/sparse.html), [advanced linear algebra](http://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class RidgeRegressionOptimize(RidgeRegressionGradient):\n",
    "    \n",
    "    def __init__(self, alpha=0, method=None):\n",
    "        \"\"\"Here's a new hyper-parameter: the optimization algorithm.\"\"\"\n",
    "        super().__init__(alpha)\n",
    "        self.method = method\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fitted with a general purpose optimization algorithm.\"\"\"\n",
    "        n, d = X.shape\n",
    "        self.b = np.mean(y)\n",
    "        \n",
    "        # Objective and gradient w.r.t. the variable to be optimized.\n",
    "        f = lambda w: self.loss(X, y, w)\n",
    "        jac = lambda w: self.grad(X, y, w)\n",
    "        \n",
    "        # Solve the problem !\n",
    "        from scipy.optimize import minimize\n",
    "        w0 = np.random.normal(size=d)\n",
    "        res = minimize(f, w0, method=self.method, jac=jac)\n",
    "        self.w = res.x\n",
    "\n",
    "models.append(evaluate(RidgeRegressionOptimize(alpha, method='Nelder-Mead')))\n",
    "models.append(evaluate(RidgeRegressionOptimize(alpha, method='BFGS')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy may be lower (depending on the random initialization) as the optimization may not have converged to the global minima. Training time is however much longer ! Especially for gradient-less optimizers such as Nelder-Mead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 More interactivity\n",
    "\n",
    "Interlude: the interactivity of Jupyter notebooks can be pushed forward with [IPython widgets](https://ipywidgets.readthedocs.io). Below, we construct a slider for the model hyper-parameter $\\alpha$, which will train the model and print its performance at each change of the value. Handy when exploring the effects of hyper-parameters ! Although it's less usefull if the required computations are long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "slider = ipywidgets.widgets.FloatSlider(\n",
    "    value=-2,\n",
    "    min=-4,\n",
    "    max=2,\n",
    "    step=1,\n",
    "    description='log(alpha) / n',\n",
    ")\n",
    "\n",
    "def handle(change):\n",
    "    \"\"\"Handler for value change: fit model and print performance.\"\"\"\n",
    "    value = change['new']\n",
    "    alpha = np.power(10, value) * n\n",
    "    clear_output()\n",
    "    print('alpha = {:.2e}'.format(alpha))\n",
    "    evaluate(RidgeRegression(alpha))\n",
    "\n",
    "slider.observe(handle, names='value')\n",
    "display(slider)\n",
    "\n",
    "slider.value = 1  # As if someone moved the slider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Machine Learning made easier\n",
    "\n",
    "Tired of writing algorithms ? Try [scikit-learn](http://scikit-learn.org), which provides many ML algorithms and related tools, e.g. metrics, cross-validation, model selection, feature extraction, pre-processing, for [predictive modeling](https://en.wikipedia.org/wiki/Predictive_modelling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "\n",
    "# The previously developed model: Ridge Regression.\n",
    "model = linear_model.RidgeClassifier(alpha)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "models.append(model)\n",
    "\n",
    "# Evaluate the predictions with a metric: the classification accuracy.\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print('accuracy: {:.2f}%'.format(acc*100))\n",
    "\n",
    "# It does indeed learn the same parameters.\n",
    "assert np.allclose(models[-1].coef_, models[0].w, rtol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's try another model !\n",
    "models.append(linear_model.LogisticRegression())\n",
    "models[-1].fit(X_train, y_train)\n",
    "acc = models[-1].score(X_test, y_test)\n",
    "print('accuracy: {:.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Deep Learning (DL)\n",
    "\n",
    "Of course ! We got two low-level Python libraries: (1) [TensorFlow](https://www.tensorflow.org/) and (2) [Theano](http://deeplearning.net/software/theano/). Both of them treat data as tensors and construct a computational graph ([dataflow paradigm](https://en.wikipedia.org/wiki/Dataflow_programming)), composed of any mathematical expressions, that get evaluated on CPUs or GPUs. Theano is the pioneer and features an optimizing compiler which will turn the computational graph into efficient code. TensorFlow has a cleaner API (not need to define expressions as strings) and does not require a compilation step (which is painful when developing models).\n",
    "\n",
    "While you'll only use Theano / TensorFlow to develop DL models, these are the higher-level libraries you'll use to define and test DL architectures on your problem:\n",
    "* [Keras](https://keras.io/): TensorFlow & Theano backends\n",
    "* [Lasagne](http://lasagne.readthedocs.io): Theano backend\n",
    "* [nolearn](https://github.com/dnouri/nolearn): sklearn-like abstraction of Lasagne\n",
    "* [Blocks](http://blocks.readthedocs.io): Theano backend\n",
    "* [TFLearn](http://tflearn.org): TensorFlow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "\n",
    "class NeuralNet(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Define Neural Network architecture.\"\"\"\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.Dense(output_dim=46, input_dim=23, activation='relu'))\n",
    "        self.model.add(keras.layers.Dense(output_dim=1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = y / 2 + 0.5  # [-1,1] -> [0,1]\n",
    "        self.model.fit(X, y, nb_epoch=5, batch_size=32)\n",
    "\n",
    "    def predict(self, X):\n",
    "        classes = self.model.predict_classes(X, batch_size=32)\n",
    "        return classes[:,0] * 2 - 1\n",
    "        \n",
    "models.append(NeuralNet())\n",
    "models[-1].fit(X_train, y_train)\n",
    "\n",
    "loss_acc = models[-1].model.evaluate(X_test, y_test/2+0.5, batch_size=32)\n",
    "print('\\n\\nTesting set: {}'.format(loss_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Evaluation\n",
    "\n",
    "Now that we tried several predictive models, it is time to evaluate them with our chosen metrics and choose the one best suited to our particular problem. Let's plot the *classification accuracy* and the *prediction time* for each classifier with [matplotlib](http://matplotlib.org), the goto 2D plotting library for scientific Python. Its API is similar to matlab.\n",
    "\n",
    "Result: The NeuralNet gives the best accuracy, by a small margin over the much simple logistic regression, but is the slowest method. Which to choose ? Again, it depends on your priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Or notebook for interaction.\n",
    "\n",
    "names, acc, times = [], [], []\n",
    "for model in models:\n",
    "    import time\n",
    "    t = time.process_time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    times.append((time.process_time()-t) * 1000)\n",
    "    acc.append(accuracy(y_pred, y_test) * 100)\n",
    "    names.append(type(model).__name__)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(acc, '.', markersize=20)\n",
    "plt.title('Accuracy [%]')\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(times, '.', markersize=20)\n",
    "plt.title('Prediction time [ms]')\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
