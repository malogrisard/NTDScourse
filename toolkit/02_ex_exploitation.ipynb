{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Python Tour of Data Science: Data Acquisition & Exploration     \n",
    "\n",
    "[MichaÃ«l Defferrard](http://deff.ch), *PhD student*, [EPFL](http://epfl.ch) [LTS2](http://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: problem definition\n",
    "\n",
    "Theme of the exercise: **understand the impact of your communication on social networks**. A real life situation: the marketing team needs help in identifying which were the most engaging posts they made on social platforms to prepare their next [AdWords](https://www.google.com/adwords/) campaign.\n",
    "\n",
    "This notebook is the second part of the exercise. Given the data we collected from Facebook an Twitter in the last exercise, we will construct an ML model and evaluate how good it is to predict the number of likes of a post / tweet given the content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Data importation\n",
    "\n",
    "1. Use `pandas` to import the `facebook.sqlite` and `twitter.sqlite` databases.\n",
    "2. Print the 5 first rows of both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is a <class 'pandas.core.frame.DataFrame'> with 52 samples of dimensionality 6.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import os.path\n",
    "\n",
    "folder = os.path.join('..', 'data', 'social_media')\n",
    "\n",
    "# Your code here.\n",
    "fb = pd.read_sql('facebook', 'sqlite:///' + os.path.join(folder, 'facebook.sqlite'))\n",
    "tw = pd.read_sql('twitter', 'sqlite:///' + os.path.join(folder, 'twitter.sqlite'))\n",
    "\n",
    "n, d = fb.shape\n",
    "print('The data is a {} with {} samples of dimensionality {}.'.format(type(fb), n, d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Vectorization\n",
    "\n",
    "First step: transform the data into a format understandable by the machine. What to do with text ? A common choice is the so-called *bag-of-word* model, where we represent each word a an integer and simply count the number of appearances of a word into a document.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's say we have a vocabulary represented by the following correspondance table.\n",
    "\n",
    "| Integer | Word    |\n",
    "|:-------:|---------|\n",
    "|    0    | unknown |\n",
    "|    1    | dog     |\n",
    "|    2    | school  |\n",
    "|    3    | cat     |\n",
    "|    4    | house   |\n",
    "|    5    | work    |\n",
    "|    6    | animal  |\n",
    "\n",
    "Then we can represent the following document\n",
    "> I have a cat. Cats are my preferred animals.\n",
    "\n",
    "by the vector $x = [6, 0, 0, 2, 0, 0, 1]^T$.\n",
    "\n",
    "**Tasks**\n",
    "\n",
    "1. Construct a vocabulary of the 100 most occuring words in your dataset.\n",
    "2. Build a vector $x \\in \\mathbb{R}^{100}$ for each document (post or tweet).\n",
    "\n",
    "Tip: the natural language modeling libraries [nltk](http://www.nltk.org/) and [gensim](https://radimrehurek.com/gensim/) are useful for advanced operations. You don't need them here.\n",
    "\n",
    "Arise a first *data cleaning* question. We may have some text in french and other in english. What do we do ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-7396ddbc48a2>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-7396ddbc48a2>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Data cleaning\n",
    "for i in range(len(fb)):\n",
    "    if fb['text'][[i] == 'http' :#or fb[i] == 'the':\n",
    "        fb.icol(i)\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "\n",
    "nwords = 100\n",
    "\n",
    "# Your code here.\n",
    "vectorizer = CountVectorizer(max_features = nwords)\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "fb_text_vec = vectorizer.fit_transform(fb['text'])\n",
    "fb_text_vectorized = fb_text_vec.toarray()\n",
    "fb_words = vectorizer.get_feature_names()\n",
    "\n",
    "#data cleaning\n",
    "fb_words.remove('http')\n",
    "\n",
    "freqs = [(word, fb_text_vec.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "fb_Most_used = sorted(freqs, key = lambda x: -x[1])\n",
    "\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "tw_text_vec = vectorizer.fit_transform(tw['text'])\n",
    "tw_text_vectorized = tw_text_vec.toarray()\n",
    "tw_words = vectorizer.get_feature_names()\n",
    "\n",
    "#data cleaning\n",
    "tw_words.remove('rt')\n",
    "\n",
    "freqs = [(word, tw_text_vec.getcol(idx).sum()) for word, idx in vectorizer.vocabulary_.items()]\n",
    "tw_Most_used = sorted(freqs, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration question: what are the 5 most used words ? Exploring your data while playing with it is a useful sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 39), ('technis', 33), ('to', 24), ('http', 21), ('and', 20)]\n",
      "[('co', 72), ('https', 66), ('mytechnis', 43), ('rt', 42), ('the', 26)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = vectorizer.vocabulary_.get('2016')\n",
    "print(fb_Most_used[:5])\n",
    "print(tw_Most_used[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Pre-processing\n",
    "\n",
    "1. The independant variables $X$ are the bags of words.\n",
    "2. The target $y$ is the number of likes.\n",
    "3. Split in half for training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "X = tw_text_vectorized\n",
    "X = X.astype(np.float)\n",
    "#X -= X.mean(axis=0)\n",
    "#X /= X.std(axis=0)\n",
    "y = tw['likes'] \n",
    "y = y.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: 38 testing and 39 training samples\n"
     ]
    }
   ],
   "source": [
    "# Training and testing sets.\n",
    "test_size = round(len(X)/2)\n",
    "print('Split: {} testing and {} training samples'.format(test_size, y.size - test_size))\n",
    "perm = np.random.permutation(y.size)\n",
    "X_test  = X[:test_size]\n",
    "X_train = X[test_size:]\n",
    "y_test  = y[perm[:test_size]]\n",
    "y_train = y[perm[test_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Linear regression\n",
    "\n",
    "Using `numpy`, fit and evaluate the linear model $$\\hat{w}, \\hat{b} = \\operatorname*{arg min}_{w,b} \\| Xw + b - y \\|_2^2.$$\n",
    "\n",
    "Please define a class `LinearRegression` with two methods:\n",
    "1. `fit` learn the parameters $w$ and $b$ of the model given the training examples.\n",
    "2. `predict` gives the estimated number of likes of a post / tweet. That will be used to evaluate the model on the testing set.\n",
    "\n",
    "To evaluate the classifier, create an `accuracy(y_pred, y_true)` function which computes the mean squared error $\\frac1n \\| \\hat{y} - y \\|_2^2$.\n",
    "\n",
    "Hint: you may want to use the function `scipy.sparse.linalg.spsolve()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "class RidgeRegression(object):\n",
    "    \"\"\"Our ML model.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        \"The class' constructor. Initialize the hyper-parameters.\"\n",
    "        self.a = alpha\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the predicted class given the features.\"\"\"\n",
    "        return np.sign(X.dot(self.w) + self.b)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Learn the model's parameters given the training data, the closed-form way.\"\"\"\n",
    "        n, d = X.shape\n",
    "        self.b = np.mean(y)\n",
    "        Ainv = np.linalg.inv(X.T.dot(X) + self.a * np.identity(d))\n",
    "        self.w = Ainv.dot(X.T).dot(y - self.b)\n",
    "\n",
    "    def loss(self, X, y, w=None, b=None):\n",
    "        \"\"\"Return the current loss.\n",
    "        This method is not strictly necessary, but it provides\n",
    "        information on the convergence of the learning process.\"\"\"\n",
    "        w = self.w if w is None else w  # The ternary conditional operator\n",
    "        b = self.b if b is None else b  # makes those tests concise.\n",
    "        import autograd.numpy as np  # See below for autograd.\n",
    "        return np.linalg.norm(np.dot(X, w) + b - y)**2 + self.a * np.linalg.norm(w, 2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation: what are the most important words a post / tweet should include ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-faffdefb4c8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mneigh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRidgeRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_predTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c3010422d64f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mAinv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAinv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/malogrisard/anaconda/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/malogrisard/anaconda/lib/python3.5/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Your code here.\n",
    "import sklearn.metrics\n",
    "\n",
    "neigh = RidgeRegression()\n",
    "neigh.fit(X_train, y_train) \n",
    "y_predTest = neigh.predict(X_train)\n",
    "train_accuracy = sklearn.metrics.accuracy_score(y_test, y_predTest)\n",
    "y_pred = neigh.predict(X_test)\n",
    "test_accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "print('train_accuracy',train_accuracy)\n",
    "print('test_accuracy',test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Interactivity\n",
    "\n",
    "1. Create a slider for the number of words, i.e. the dimensionality of the samples $x$.\n",
    "2. Print the accuracy for each change on the slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Scikit learn\n",
    "\n",
    "1. Fit and evaluate the linear regression model using `sklearn`.\n",
    "2. Evaluate the model with the mean squared error metric provided by `sklearn`.\n",
    "3. Compare with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy 0.948717948718\n",
      "test_accuracy 0.684210526316\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "\n",
    "# Your code here.\n",
    "neigh = sklearn.linear_model.LogisticRegression()\n",
    "neigh.fit(X_train, y_train) \n",
    "y_predTest = neigh.predict(X_train)\n",
    "train_accuracy = sklearn.metrics.accuracy_score(y_train, y_predTest)\n",
    "y_pred = neigh.predict(X_test)\n",
    "test_accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "print('train_accuracy',train_accuracy)\n",
    "print('test_accuracy',test_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Deep Learning\n",
    "\n",
    "Try a simple deep learning model !\n",
    "\n",
    "Another modeling choice would be to use a Recurrent Neural Network (RNN) and feed it the sentence words after words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'  # tensorflow\n",
    "import keras\n",
    "\n",
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Evaluation\n",
    "\n",
    "Use [matplotlib](http://matplotlib.org) to plot a performance visualization. E.g. the true number of likes and the real number of likes for all posts / tweets.\n",
    "\n",
    "What do you observe ? What are your suggestions to improve the performance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
